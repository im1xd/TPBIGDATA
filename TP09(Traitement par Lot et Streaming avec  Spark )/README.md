# TP09: Batch & Streaming Processing with Apache Spark

This project demonstrates Big Data processing using **Apache Spark** (Batch & Streaming) within a **Dockerized Hadoop Ecosystem**. It was implemented as part of the Big Data module.

## ğŸ“Œ Overview
We implemented three core applications using **Java**:
1.  **Batch Processing:** A classic WordCount application.
2.  **Stream Processing:** A real-time WordCount using Netcat and Spark Streaming.
3.  **Data Analytics (Bonus):** Analyzing restaurant data to determine cuisine popularity using `restaurants.csv`.

## ğŸ› ï¸ Technologies & Tools
* **Apache Spark 3.0.0** (Core & Streaming)
* **Hadoop HDFS** (Simulated environment)
* **Java 8** (OpenJDK)
* **Maven** (Build automation)
* **Docker & Docker Compose** (Containerization)
* **Netcat** (Data stream simulation)

---

## ğŸš€ Environment Setup
The cluster consists of one Master node and two Worker nodes managed via Docker Compose.

### 1. Start the Cluster
```bash
docker-compose up -d
docker exec -it hadoop-master bash
