# -*- coding: utf-8 -*-
"""TP2bigdata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTQAWsx4R2e_l0_U7lH3swjv-5bRIbxf
"""

!pip install kaggle
from google.colab import files
files.upload()

!mkdir ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store -p /content/data
!unzip /content/data/ecommerce-behavior-data-from-multi-category-store.zip -d /content/data

DATA_DIR = "/data"

import os, pandas as pd

DATA_DIR = "/content/data"
csv_files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".csv")]
print("Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©:", csv_files)

sample = pd.read_csv(csv_files[0], nrows=5)
print("Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print(sample.columns.tolist())
sample.head()

"""ğŸ”¹ Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª (chunks) Ù„ØªØ¬Ù†Ù‘Ø¨ Ù†ÙØ§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""

import pandas as pd
import gc
from collections import Counter
import time

file_path = "/content/data/2019-Oct.csv"

chunksize = 2_000_000
event_counter = Counter()
start = time.time()

for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):
    event_counter.update(chunk["event_type"].values)
    del chunk
    gc.collect()

end = time.time()
print("Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ØªÙƒØ±Ø§Ø±Ø§Ù‹:")
print(event_counter.most_common())
print(f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚: {end - start:.2f} Ø«Ø§Ù†ÙŠØ©")

"""# Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· â€” Ø£ÙƒØ«Ø± Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© (brands) ØªÙƒØ±Ø§Ø±Ø§Ù‹ Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ø­Ø¯Ø«"""

import pandas as pd
import gc

file_path = "/content/data/2019-Oct.csv"
chunksize = 1_000_000

result = {}

for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):
    for event_type, df_group in chunk.groupby("event_type"):
        top_brands = (
            df_group["brand"].value_counts().head(3).to_dict()
        )
        if event_type not in result:
            result[event_type] = Counter()
        result[event_type].update(top_brands)
    del chunk
    gc.collect()

for event_type, brands in result.items():
    print(f"\nØ§Ù„Ø­Ø¯Ø«: {event_type}")
    print("Ø£Ø´Ù‡Ø± Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ©:", dict(brands.most_common(5)))

"""# Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Parquet (Ø§Ø®ØªÙŠØ§Ø±ÙŠ  Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ )"""

import dask.dataframe as dd
import time

start = time.time()
ddf = dd.read_csv("/content/data/*.csv", assume_missing=True, blocksize="64MB")
ddf.to_parquet("/content/data/parquet_data", engine="pyarrow", compression="snappy")
end = time.time()

print(f"ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet Ø®Ù„Ø§Ù„ {end - start:.1f} Ø«Ø§Ù†ÙŠØ©.")

"""#  Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask (Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©)"""

import dask.dataframe as dd
from dask.diagnostics import ProgressBar

ddf = dd.read_csv("/content/data/*.csv", assume_missing=True, blocksize="64MB")
print("Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:", list(ddf.columns))

with ProgressBar():
    event_counts = ddf["event_type"].value_counts().compute()

print("\nØ¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«:")
print(event_counts)

with ProgressBar():
    avg_price = ddf.groupby("event_type")["price"].mean().compute()

print("\nÙ…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø¹Ø± Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ø­Ø¯Ø«:")
print(avg_price)

"""# 1. ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø¹Ù„Ù‰ Ù…Ø¯Ø§Ø± Ø§Ù„ÙŠÙˆÙ… (Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)"""

import dask.dataframe as dd
from dask.diagnostics import ProgressBar

ddf = dd.read_parquet("/content/data/parquet_data")

ddf["event_time"] = dd.to_datetime(ddf["event_time"], errors="coerce")
ddf["hour"] = ddf["event_time"].dt.hour

with ProgressBar():
    hourly = ddf.groupby("hour")["event_type"].value_counts().compute().unstack(fill_value=0)

hourly.plot(kind="line", figsize=(10,5), title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„Ø³Ø§Ø¹Ø© (view/cart/purchase)")

"""# 2. Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª ÙÙ‚Ø·"""

import pandas as pd

with ProgressBar():
    purchase_df = ddf[ddf["event_type"] == "purchase"]
    brand_avg_price = purchase_df.groupby("brand")["price"].mean().compute().sort_values(ascending=False).head(10)

print("Ø£ØºÙ„Ù‰ 10 Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¬Ø§Ø±ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª:")
print(brand_avg_price)

"""# 3. Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„ (Conversion Rate)
Ø£ÙŠ: ÙƒÙ… Ù…Ù† Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ØªØªØ­ÙˆÙ„ Ø¥Ù„Ù‰ Ø¹Ù…Ù„ÙŠØ§Øª Ø´Ø±Ø§Ø¡ØŸ
"""

with ProgressBar():
    views = ddf[ddf["event_type"] == "view"]["product_id"].nunique().compute()
    purchases = ddf[ddf["event_type"] == "purchase"]["product_id"].nunique().compute()

conversion_rate = (purchases / views) * 100
print(f"Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù† Ù…Ø´Ø§Ù‡Ø¯Ø© Ø¥Ù„Ù‰ Ø´Ø±Ø§Ø¡: {conversion_rate:.4f}%")

"""# 4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± Ø¬Ø°Ø¨Ù‹Ø§ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø´Ø±Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø©)"""

with ProgressBar():
    views_per_brand = ddf[ddf["event_type"] == "view"].groupby("brand")["user_id"].count().compute()
    purchases_per_brand = ddf[ddf["event_type"] == "purchase"].groupby("brand")["user_id"].count().compute()

brand_conversion = (purchases_per_brand / views_per_brand * 100).dropna().sort_values(ascending=False).head(10)
print("Ø£ÙƒØ«Ø± Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ù…Ù† Ø­ÙŠØ« Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:")
print(brand_conversion)

"""CSV"""

import os
import dask.dataframe as dd
import pandas as pd
from dask.diagnostics import ProgressBar


DATA_PARQUET = "/content/data/parquet_data"
RESULTS_DIR = "/content/results"
os.makedirs(RESULTS_DIR, exist_ok=True)

print("donload data ")
ddf = dd.read_parquet(DATA_PARQUET)




ddf["event_time"] = dd.to_datetime(ddf["event_time"], errors="coerce")






with ProgressBar():
    event_counts = ddf["event_type"].value_counts().compute()
event_counts.to_csv(f"{RESULTS_DIR}/event_counts.csv", header=["count"])
print(" event_counts.csv  ")






with ProgressBar():
    avg_price = ddf.groupby("event_type")["price"].mean().compute()
avg_price.to_csv(f"{RESULTS_DIR}/avg_price_per_event.csv", header=["avg_price"])
print(" avg_price_per_event.csv  ")





with ProgressBar():
    top_brands = (
        ddf.groupby(["event_type", "brand"])["user_id"]
        .count()
        .compute()
        .reset_index()
        .rename(columns={"user_id": "count"})
    )
top_brands = (
    top_brands.sort_values(["event_type", "count"], ascending=[True, False])
    .groupby("event_type")
    .head(10)
)
top_brands.to_csv(f"{RESULTS_DIR}/top_brands_by_event.csv", index=False)
print("âœ” top_brands_by_event.csv  ")





ddf["hour"] = ddf["event_time"].dt.hour
with ProgressBar():
    hourly = ddf.groupby("hour")["event_type"].value_counts().compute().unstack(fill_value=0)
hourly.to_csv(f"{RESULTS_DIR}/hourly_event_distribution.csv")
print(" hourly_event_distribution.csv  ")





with ProgressBar():
    views = ddf[ddf["event_type"] == "view"]["product_id"].nunique().compute()
    purchases = ddf[ddf["event_type"] == "purchase"]["product_id"].nunique().compute()
conversion_rate = (purchases / views) * 100
pd.DataFrame(
    {"views": [views], "purchases": [purchases], "conversion_rate(%)": [conversion_rate]}
).to_csv(f"{RESULTS_DIR}/conversion_rate.csv", index=False)
print(" conversion_rate.csv  ")





with ProgressBar():
    views_per_brand = ddf[ddf["event_type"] == "view"].groupby("brand")["user_id"].count().compute()
    purchases_per_brand = ddf[ddf["event_type"] == "purchase"].groupby("brand")["user_id"].count().compute()

brand_conversion = (
    (purchases_per_brand / views_per_brand * 100)
    .dropna()
    .sort_values(ascending=False)
    .reset_index()
)
brand_conversion.columns = ["brand", "conversion_rate(%)"]
brand_conversion.to_csv(f"{RESULTS_DIR}/brand_conversion_rate.csv", index=False)
print(" brand_conversion_rate.csv ØªÙ… Ø­ÙØ¸Ù‡")


print("\n ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ:", RESULTS_DIR)
print("Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:")
for f in os.listdir(RESULTS_DIR):
    print(" -", f)

"""# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""

import os, time, gc, pandas as pd
import dask.dataframe as dd
import matplotlib.pyplot as plt
from dask.diagnostics import ProgressBar

DATA_DIR = "/content/data"
RESULTS_DIR = "/content/results"
os.makedirs(RESULTS_DIR, exist_ok=True)

def dir_size(path):
    if os.path.isfile(path):
        return os.path.getsize(path)
    total = 0
    for dirpath, _, filenames in os.walk(path):
        for f in filenames:
            total += os.path.getsize(os.path.join(dirpath, f))
    return total


import pandas as pd
from collections import Counter

pandas_start = time.time()
event_counter = Counter()
chunksize = 2_000_000
file_path = os.path.join(DATA_DIR, "2019-Oct.csv")

for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):
    event_counter.update(chunk["event_type"].values)
    del chunk
    gc.collect()

pandas_end = time.time()
pandas_time = pandas_end - pandas_start
pandas_size = dir_size(file_path)

print(f" pandas + chunksize done in {pandas_time:.2f} s")


dask_start = time.time()
with ProgressBar():
    ddf = dd.read_csv(os.path.join(DATA_DIR, "*.csv"), assume_missing=True, blocksize="64MB")
    _ = ddf["event_type"].value_counts().compute()
dask_end = time.time()
dask_time = dask_end - dask_start
dask_size = dir_size(DATA_DIR)
print(f" Dask done in {dask_time:.2f} s")


parquet_dir = os.path.join(DATA_DIR, "parquet_data_auto")
parquet_start = time.time()
ddf.to_parquet(parquet_dir, engine="pyarrow", compression="snappy", write_index=False)
parquet_end = time.time()
parquet_time = parquet_end - parquet_start
parquet_size = dir_size(parquet_dir)
print(f" Parquet (compressed) done in {parquet_time:.2f} s")

comparison = pd.DataFrame({
    "Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©": ["pandas + chunksize", "Dask", "Parquet (compressed)"],
    "Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ù‚ (Ø«Ø§Ù†ÙŠØ©)": [pandas_time, dask_time, parquet_time],
    "Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (MB)": [pandas_size/1e6, dask_size/1e6, parquet_size/1e6],
    "Ø§Ù„Ù…Ø²Ø§ÙŠØ§": [
        "ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø£ÙŠ Ø¬Ù‡Ø§Ø² - Ù„Ø§ ØªØ³Ø¨Ø¨ Out of Memory",
        "ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ø®Ù…Ø© Ø¨Ø³Ù‡ÙˆÙ„Ø© ÙˆØ³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©",
        "ØªØ­Ø³Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† ÙˆØªØ³Ø±Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©"
    ],
    "Ø§Ù„Ø¹ÙŠÙˆØ¨": [
        "ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø­Ù„Ù‚Ø© for ÙˆØªÙƒØ±Ø§Ø± ÙŠØ¯ÙˆÙŠ",
        "ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…ÙƒØªØ¨Ø© Ø¥Ø¶Ø§ÙÙŠØ© Dask",
        "ØªØ³ØªÙ‡Ù„Ùƒ ÙˆÙ‚Øª Ø£ÙˆÙ„ÙŠ ÙƒØ¨ÙŠØ± Ù„Ù„ØªØ­ÙˆÙŠÙ„"
    ]
})

comparison.to_csv(f"{RESULTS_DIR}/methods_comparison.csv", index=False, encoding='utf-8-sig')
print("\n ØªÙ… Ø­ÙØ¸ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙÙŠ:", f"{RESULTS_DIR}/methods_comparison.csv")


plt.figure(figsize=(7,4))
plt.bar(comparison["Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©"], comparison["Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ù‚ (Ø«Ø§Ù†ÙŠØ©)"], color=['skyblue','orange','green'])
plt.title("Ù…Ù‚Ø§Ø±Ù†Ø© Ø²Ù…Ù† Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨ÙŠÙ† Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø«Ù„Ø§Ø«")
plt.ylabel("Ø§Ù„Ø²Ù…Ù† (Ø«Ø§Ù†ÙŠØ©)")
plt.xticks(rotation=15)
plt.tight_layout()
plt.savefig(f"{RESULTS_DIR}/methods_comparison.png")
plt.show()


print(" ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ  ÙÙŠ:", f"{RESULTS_DIR}/methods_comparison.png")

display(comparison)