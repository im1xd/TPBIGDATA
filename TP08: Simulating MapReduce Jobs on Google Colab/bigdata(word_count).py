# -*- coding: utf-8 -*-
"""BigData(Word Count).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IHv9MvEuPX2z6dUEY3avK9duJtfRmCBV

**Create data.txt**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile data.txt
# spark makes big data easy
# big data needs spark power
# python is great for data processing

"""**dala mapper**"""

def mapper(line):
    words = line.strip().split()
    return [(word, 1) for word in words]
# semple exemple
print(mapper("big data needs spark power"))

"""**shuffle tajmi3**"""

from collections import defaultdict

def shuffle(mapped_data):
    grouped = defaultdict(list)
    for key, value in mapped_data:
        grouped[key].append(value)
    return grouped

"""**reducer (التجميع النهائي)**"""

def reducer(grouped_data):
    reduced = {}
    for key, values in grouped_data.items():
        reduced[key] = sum(values)
    return reduced

"""**5 mapreduce**"""

def mapreduce(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()

    # Map
    mapped = []
    for line in lines:
        mapped.extend(mapper(line))

    print("Mapped Output (first 10):", mapped[:10])

    # Shuffle
    grouped = shuffle(mapped)
    print("\nGrouped Data (sample):", {k: grouped[k] for k in list(grouped)[:5]})

    # Reduce
    reduced = reducer(grouped)
    return reduced

results = mapreduce("data.txt")
print("\nFinal Results:")
for word, count in sorted(results.items()):
    print(f"{word}: {count}")